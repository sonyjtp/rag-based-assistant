# ğŸ¤– RAG-Based AI Assistant - AAIDC Project

> A production-ready Retrieval-Augmented Generation (RAG) chatbot that answers questions exclusively from a set of custom documents using LangChain, ChromaDB, and multiple LLM providers.

[![Python Version](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/license-CC%20BY--NC--SA%204.0-blue.svg)](LICENSE)
[![Tests](https://img.shields.io/badge/tests-175%20passing-brightgreen.svg)]()
[[[[[![Code Coverage](https://img.shields.io/badge/coverage-91.28%25-brightgreen.svg)]()
[![Pylint](https://github.com/sonyjtp/rag-based-assistant/actions/workflows/pylint.yml/badge.svg)](https://github.com/sonyjtp/rag-based-assistant/actions/workflows/pylint.yml)

[Quick Start](#-quick-start) â€¢ [Features](#-features) â€¢ [Installation](#-installation) â€¢ [Contributing](#-contributing)


---

## ğŸ“‹ Table of Contents

- [Overview](#-overview)
- [Features](#-features)
- [Quick Start](#-quick-start)
- [Installation](#-installation)
- [Configuration](#-configuration)
- [Usage](#-usage)
- [Project Architecture](#-project-architecture)
- [Project Structure](#-project-structure)
- [Testing](#-testing)
- [Customization Guide](#-customization-guide)
- [Memory Management](#-memory-management)
- [Reasoning Strategies](#-reasoning-strategies)
- [Troubleshooting](#-troubleshooting)
- [Contributing](#-contributing)
- [License](#-license)

---

## ğŸ¯ Overview

This project implements a **Retrieval-Augmented Generation (RAG)** chatbot that:

- ğŸ“š **Loads custom documents** from your `data/` directory
- ğŸ” **Chunks and embeds** text using advanced text splitting strategies
- ğŸ’¾ **Stores vectors** in ChromaDB vector database
- ğŸ¤ **Answers questions** exclusively from your documents
- ğŸ§  **Maintains conversation** memory across multiple interactions
- ğŸ”Œ **Supports multiple LLMs**: OpenAI, Groq, Google Gemini
- ğŸ›¡ï¸ **Prevents hallucination** with strict prompt constraints
- ğŸ“Š **Tracks reasoning** with configurable strategies

**Key Constraint**: The assistant **only answers questions based on the provided documents**. Questions that cannot be answered from the documents are rejected with: *"I'm sorry, that information is not known to me."*

---

## âœ¨ Features

### Core RAG Capabilities
- âœ… Document loading from text files
- âœ… Intelligent text chunking with overlap
- âœ… Semantic search using embeddings
- âœ… Context-aware question answering
- âœ… Document metadata preservation (title, tags, filename)

### Memory Management
- âœ… **Buffer Memory**: Stores full conversation history
- âœ… **Sliding Window Memory**: Keeps recent messages + summarized history
- âœ… **Summarization**: Automatic conversation summarization when window fills
- âœ… **Memory Strategy Switching**: Change strategies on-the-fly

### LLM Integration
- âœ… **OpenAI GPT-4** / GPT-4o-mini
- âœ… **Groq Llama 3.1** (fast inference)
- âœ… **Google Gemini** Pro
- âœ… Automatic fallback to next available provider
- âœ… Device detection (CUDA, MPS, CPU)

### Reasoning Strategies
- âœ… **Chain-of-Thought**: Step-by-step reasoning
- âœ… **Tree-of-Thought**: Explores multiple reasoning paths
- âœ… **Self-Consistent**: Generates multiple outputs and votes
- âœ… Configurable via YAML

### Safety & Quality
- âœ… **Hallucination Prevention**: Strict prompt constraints
- âœ… **Input Validation**: Document and query validation
- âœ… **Error Handling**: Comprehensive exception handling
- âœ… **Logging**: Detailed logging throughout
- âœ… **191 Test Cases**: 78% code coverage

### User Interfaces
- âœ… **CLI Interface** (`app.py`): Command-line chatbot
- âœ… **Streamlit UI** (`streamlit_app.py`): Web-based interface
- âœ… **API Ready**: Can be integrated with FastAPI/Flask

---

## ğŸš€ Quick Start

### Prerequisites
- **Python 3.8+** (Tested with 3.12.12 âœ…)
- **API Key** for at least one LLM provider:
  - OpenAI: `OPENAI_API_KEY`
  - Groq: `GROQ_API_KEY`
  - Google: `GOOGLE_API_KEY`

### 1ï¸âƒ£ Clone & Setup (2 minutes)

```bash
# Clone the repository
git clone https://github.com/sonyjtp/rt-aaidc-rag-based-assistant.git
cd rt-aaidc-rag-based-assistant

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2ï¸âƒ£ Configure API Key (1 minute)

```bash
# Copy example env file
cp .env_example .env

# Edit .env with your API key
# Choose ONE provider:
# Option 1: OpenAI
OPENAI_API_KEY=your_openai_key_here

# Option 2: Groq (recommended - fast and free)
GROQ_API_KEY=your_groq_key_here

# Option 3: Google Gemini
GOOGLE_API_KEY=your_google_key_here
```

### 3ï¸âƒ£ Add Your Documents (2 minutes)

```bash
# Replace sample files in data/ with your documents
# Files should be .txt format

ls data/
# Output: your_document.txt, another_doc.txt, ...
```

### 4ï¸âƒ£ Run the Assistant (30 seconds)

**CLI Version:**
```bash
python src/app.py
```

**Web UI (Streamlit):**
```bash
streamlit run src/streamlit_app.py
```

---

## ğŸ“¦ Installation

### Full Installation with Development Tools

```bash
# Clone repository
git clone https://github.com/yourusername/rt-aaidc-rag-based-assistant.git
cd rt-aaidc-rag-based-assistant

# Create virtual environment
python -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Install development/test dependencies (optional)
pip install -r requirements-dev.txt

# Set up pre-commit hooks for automatic code formatting
pre-commit install

# Verify installation
python -c "import langchain; print('âœ“ LangChain installed')"
```

### Docker Installation (Optional)

```bash
# Build Docker image
docker build -t rag-assistant .

# Run container
docker run -e OPENAI_API_KEY=your_key -v $(pwd)/data:/app/data rag-assistant
```

---

## âš™ï¸ Configuration

### Environment Variables (.env)

```env
# LLM Configuration
OPENAI_API_KEY=sk-...
GROQ_API_KEY=gsk_...
GOOGLE_API_KEY=AIzaSy...
OPENAI_MODEL=gpt-4o-mini
GROQ_MODEL=llama-3.1-8b-instant

# Vector Database
CHROMA_API_KEY=your_api_key
CHROMA_TENANT=default
CHROMA_DATABASE=default

# Embedding Model
VECTOR_DB_EMBEDDING_MODEL=sentence-transformers/all-mpnet-base-v2

# Memory Strategy
MEMORY_STRATEGY=conversation_buffer_memory  # or summarization_sliding_window

# Retrieval
RETRIEVAL_K=5  # Number of documents to retrieve

# Text Processing
CHUNK_SIZE=1000
CHUNK_OVERLAP=200

# Reasoning Strategy
REASONING_STRATEGY=chain_of_thought
```

### Configuration Files

**config.py** - Core configuration
```python
CHUNK_SIZE_DEFAULT = 1000
CHUNK_OVERLAP_DEFAULT = 200
RETRIEVAL_K_DEFAULT = 5
```

**config/prompt-config.yaml** - System prompts and constraints
```yaml
system_prompts:
  - "Only answer based on provided documents"
  - "Do not use training data or general knowledge"
  - "If information not found: respond with 'I'm sorry, that information is not known to me.'"
```

**config/memory_strategies.yaml** - Memory configuration
```yaml
memory_strategies:
  conversation_buffer_memory:
    enabled: true
    parameters:
      memory_key: chat_history
  summarization_sliding_window:
    enabled: true
    parameters:
      window_size: 5
      memory_key: chat_history
```

**config/reasoning_strategies.yaml** - Reasoning approaches
```yaml
reasoning_strategies:
  chain_of_thought:
    enabled: true
    instructions: "Think step by step..."
  tree_of_thought:
    enabled: true
    instructions: "Explore multiple paths..."
```

---

## ğŸ’¬ Usage

### CLI Usage

```bash
python src/app.py

# Prompts you to ask questions
# Type 'quit' to exit

> What is the main topic of the documents?
Assistant: Based on the documents, the main topics are...

> Tell me more
Assistant: [Provides additional context from memory]

> quit
Goodbye!
```

### Streamlit Web Interface

```bash
streamlit run src/streamlit_app.py

# Opens http://localhost:8501
# - Sidebar: Clear history, configure settings
# - Main: Chat interface
# - Auto-saves conversation
```

### Python API

```python
from src.rag_assistant import RAGAssistant

# Initialize assistant
assistant = RAGAssistant()

# Add documents
documents = [
    {"content": "Document text...", "title": "Doc 1", "filename": "doc1.txt"}
]
assistant.add_documents(documents)

# Ask questions
response = assistant.invoke("What is the main topic?")
print(response)

# Get memory history
memory_vars = assistant.memory_manager.get_memory_variables()
print(memory_vars["chat_history"])
```

---

## ğŸ—ï¸ Project Architecture

### System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    User Interface                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚   CLI App    â”‚      â”‚  Streamlit   â”‚                â”‚
â”‚  â”‚  (app.py)    â”‚      â”‚    (web UI)  â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚                            â”‚
                â–¼                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  RAG Assistant Core                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  RAGAssistant                                    â”‚  â”‚
â”‚  â”‚  - invoke(query)                                 â”‚  â”‚
â”‚  â”‚  - add_documents(docs)                           â”‚  â”‚
â”‚  â”‚  - retrieve_context(query, k)                    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚              â”‚              â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
    â”‚ VectorDB â”‚    â”‚   Memory   â”‚  â”‚ Prompt   â”‚
    â”‚          â”‚    â”‚  Manager   â”‚  â”‚ Builder  â”‚
    â”‚ ChromaDB â”‚    â”‚ (Buffer or â”‚  â”‚          â”‚
    â”‚          â”‚    â”‚ Summarized)â”‚  â”‚ System   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚              â”‚              â”‚
            â–¼              â–¼              â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚       LLM Integration               â”‚
    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
    â”‚ â”‚OpenAIâ”‚ â”‚Groq â”‚ â”‚  Google  â”‚      â”‚
    â”‚ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

```
User Query
    â”‚
    â–¼
Document Search (VectorDB)
    â”‚
    â”œâ”€â–º Retrieve relevant documents (k=5)
    â”‚
    â–¼
Context Building
    â”‚
    â”œâ”€â–º Combine context with history
    â”œâ”€â–º Add system prompts
    â”‚
    â–¼
LLM Processing
    â”‚
    â”œâ”€â–º Apply reasoning strategy
    â”œâ”€â–º Generate response
    â”‚
    â–¼
Memory Update
    â”‚
    â”œâ”€â–º Save to conversation history
    â”œâ”€â–º Apply memory strategy
    â”‚
    â–¼
Response to User
```

---

## ğŸ“ Project Structure

```
rt-aaidc-rag-based-assistant/
â”‚
â”œâ”€â”€ src/                          # Source code
â”‚   â”œâ”€â”€ app.py                   # CLI interface
â”‚   â”œâ”€â”€ streamlit_app.py         # Web UI
â”‚   â”œâ”€â”€ rag_assistant.py         # Core RAG logic (98% tested)
â”‚   â”œâ”€â”€ vectordb.py              # Vector database wrapper
â”‚   â”œâ”€â”€ chroma_client.py         # ChromaDB client
â”‚   â”œâ”€â”€ embeddings.py            # Embedding model initialization
â”‚   â”œâ”€â”€ llm_utils.py             # LLM provider selection
â”‚   â”œâ”€â”€ prompt_builder.py        # Prompt generation (97% tested)
â”‚   â”œâ”€â”€ memory_manager.py        # Memory handling (81% tested)
â”‚   â”œâ”€â”€ sliding_window_memory.py # Summarization-based memory (90% tested)
â”‚   â”œâ”€â”€ reasoning_strategy_loader.py  # Reasoning strategies (100% tested)
â”‚   â”œâ”€â”€ file_utils.py            # File I/O utilities
â”‚   â”œâ”€â”€ config.py                # Configuration (100% tested)
â”‚   â””â”€â”€ logger.py                # Logging setup (96% tested)
â”‚
â”œâ”€â”€ config/                       # Configuration files
â”‚   â”œâ”€â”€ prompt-config.yaml       # System prompts
â”‚   â”œâ”€â”€ memory_strategies.yaml   # Memory configurations
â”‚   â””â”€â”€ reasoning_strategies.yaml # Reasoning strategies
â”‚
â”œâ”€â”€ data/                         # Document storage
â”‚   â”œâ”€â”€ sample_doc1.txt
â”‚   â”œâ”€â”€ sample_doc2.txt
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ tests/                        # Test suite (191 tests)
â”‚   â”œâ”€â”€ test_rag_assistant.py           (26 tests)
â”‚   â”œâ”€â”€ test_prompt_builder.py          (35 tests)
â”‚   â”œâ”€â”€ test_hallucination_prevention.py (15 tests)
â”‚   â”œâ”€â”€ test_memory_manager.py          (16 tests)
â”‚   â”œâ”€â”€ test_reasoning_strategy.py      (31 tests)
â”‚   â”œâ”€â”€ test_embeddings.py              (16 tests)
â”‚   â”œâ”€â”€ test_file_utils.py              (32 tests)
â”‚   â”œâ”€â”€ test_sliding_window_memory.py   (39 tests)
â”‚   â”œâ”€â”€ test_integrations.py            (20 tests)
â”‚   â””â”€â”€ test_app.py                     (5 tests)
â”‚
â”œâ”€â”€ logs/                         # Application logs
â”‚   â”œâ”€â”€ debug.log
â”‚   â””â”€â”€ rag_assistant.log
â”‚
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ requirements-test.txt         # Testing dependencies
â”œâ”€â”€ pytest.ini                    # Pytest configuration
â”œâ”€â”€ .env.example                  # Example environment variables
â”œâ”€â”€ .coveragerc                   # Coverage configuration
â”œâ”€â”€ README.md                     # This file
â”œâ”€â”€ LICENSE                       # MIT License
â””â”€â”€ .gitignore
```

---

## ğŸ§ª Testing

### Run Full Test Suite

```bash
# Run all 191 tests
pytest -v

# Run with coverage report
pytest --cov=src --cov-report=html

# View coverage report
open htmlcov/index.html
```

### Pre-Commit Testing

Before you commit, the following checks run automatically:

```bash
# Install pre-commit hooks (one-time setup)
pre-commit install

# Manual run of all checks
pre-commit run --all-files

# Pre-commit checks include:
# âœ… Code formatting (Black, isort)
# âœ… Code linting (Pylint, Flake8)
# âœ… Test coverage (minimum 90%)
```

**Note**: Commits will be rejected if test coverage drops below 90%. To bypass (not recommended):
```bash
git commit --no-verify  # Skip pre-commit hooks
```

### Coverage Requirements

- **Minimum Coverage**: 90% (enforced by pre-commit hooks)
- **Target Coverage**: 95%+
- **Critical Modules**: 100% (rag_assistant, config, reasoning_strategy_loader)

### Run Specific Tests

```bash
# Test RAG assistant
pytest tests/test_rag_assistant.py -v

# Test prompt building
pytest tests/test_prompt_builder.py -v

# Test hallucination prevention
pytest tests/test_hallucination_prevention.py -v

# Test memory management
pytest tests/test_memory_manager.py -v
```

### Coverage Badge Updates

The coverage badge in the README is automatically updated in CI/CD:

```bash
# Manual update (for local development)
python update_coverage.py

# This script:
# 1. Reads coverage.xml (generated by pytest)
# 2. Extracts coverage percentage
# 3. Updates README badge with current coverage
# 4. Colors badge based on coverage level (green/yellow/red)
```

The badge is updated:
- âœ… On every push to main (via GitHub Actions)
- âœ… Before pull requests (verify coverage meets threshold)
- âœ… Manually via `python update_coverage.py`

### Test Coverage

```
Overall Coverage: 78%
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Module                   â”‚ Coverage â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ rag_assistant.py         â”‚ 98%      â”‚
â”‚ prompt_builder.py        â”‚ 97%      â”‚
â”‚ reasoning_strategy_loaderâ”‚ 100%     â”‚
â”‚ config.py                â”‚ 100%     â”‚
â”‚ memory_manager.py        â”‚ 81%      â”‚
â”‚ sliding_window_memory.py â”‚ 90%      â”‚
â”‚ embeddings.py            â”‚ 90%      â”‚
â”‚ file_utils.py            â”‚ 90%      â”‚
â”‚ chroma_client.py         â”‚ 85%      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ›ï¸ Customization Guide

### Change Memory Strategy

```python
# In config.py or .env
MEMORY_STRATEGY = "conversation_buffer_memory"  # Or "summarization_sliding_window"

# In code
from src.memory_manager import MemoryManager
memory = MemoryManager(llm=llm, strategy="summarization_sliding_window")
```

### Switch LLM Provider

```bash
# In .env - set which API key to use
OPENAI_API_KEY=sk-...    # Uses OpenAI
# GROQ_API_KEY=...       # Commented out - won't use Groq
# GOOGLE_API_KEY=...     # Commented out - won't use Google
```

### Adjust Document Chunking

```bash
# In .env
CHUNK_SIZE=2000          # Larger chunks
CHUNK_OVERLAP=400        # More overlap for context
RETRIEVAL_K=10           # Retrieve more documents
```

### Configure Reasoning Strategy

```yaml
# In config/reasoning_strategies.yaml
reasoning_strategies:
  chain_of_thought:
    enabled: true
    instructions: "Think through this step by step..."
  tree_of_thought:
    enabled: true
    instructions: "Explore multiple reasoning paths..."
```

### Add Custom Prompts

```python
# In src/prompt_builder.py
def build_system_prompts():
    return [
        "Your custom instruction 1",
        "Your custom instruction 2",
        # ... existing prompts
    ]
```

---

## ğŸ§  Memory Management

### Buffer Memory
- **Use case**: Short conversations (< 20 messages)
- **Pros**: Remembers everything, simple
- **Cons**: Token usage grows, no summarization

```yaml
conversation_buffer_memory:
  enabled: true
  parameters:
    memory_key: chat_history
```

### Sliding Window Memory
- **Use case**: Long conversations (100+ messages)
- **Pros**: Keeps recent context, summarizes old conversations
- **Cons**: Requires LLM for summarization

```yaml
summarization_sliding_window:
  enabled: true
  parameters:
    window_size: 5        # Keep last 5 messages
    memory_key: chat_history
```

### Disable Memory
```bash
MEMORY_STRATEGY=none
```

---

## ğŸ¯ Reasoning Strategies

### Available Strategies

1. **Chain-of-Thought**
   - Step-by-step reasoning
   - Best for: Complex questions requiring multiple steps

2. **Tree-of-Thought**
   - Explores multiple reasoning paths
   - Best for: Questions with multiple valid approaches

3. **Self-Consistent**
   - Generates multiple answers, picks best
   - Best for: Ensuring consistent, reliable answers

```bash
# Set in .env
REASONING_STRATEGY=chain_of_thought
```

---

## â“ Troubleshooting

### Common Issues

#### "API Key not found"
```bash
# Solution: Check your .env file
cat .env | grep API_KEY

# Make sure the file exists and has correct keys
cp .env_example .env
# Edit .env with your actual API key
```

#### "No documents found"
```bash
# Solution: Add .txt files to data/ directory
ls data/
# Should show your document files

# Or load documents programmatically
assistant.add_documents([{"content": "...", "title": "Doc1"}])
```

#### "Out of memory / token limit exceeded"
```bash
# Solution 1: Use smaller chunk size
CHUNK_SIZE=500

# Solution 2: Reduce retrieval results
RETRIEVAL_K=3

# Solution 3: Use sliding window memory
MEMORY_STRATEGY=summarization_sliding_window
```

#### "LLM not responding / Timeout"
```bash
# Solution: Switch to faster LLM
# In .env, use Groq (fastest and free):
GROQ_API_KEY=gsk_...
# Comment out other API keys
```

### Debug Mode

```bash
# Enable detailed logging
# In logger.py, set logging level
logging.basicConfig(level=logging.DEBUG)

# Run with verbose output
pytest -v --log-cli-level=DEBUG
```

---

## ğŸ¤ Contributing

We welcome contributions! Here's how to get involved:

### Development Setup

```bash
# Fork and clone
git clone https://github.com/yourusername/rt-aaidc-rag-based-assistant.git
cd rt-aaidc-rag-based-assistant

# Create feature branch
git checkout -b feature/amazing-feature

# Install dev dependencies
pip install -r requirements-test.txt

# Make changes and run tests
pytest tests/ -v

# Commit and push
git add .
git commit -m "feat: add amazing feature"
git push origin feature/amazing-feature

# Create pull request on GitHub
```

### Testing Requirements

All contributions must include:
- âœ… Unit tests for new functionality
- âœ… Integration tests if applicable
- âœ… Documentation updates
- âœ… All tests must pass: `pytest -v`

### Code Style

- Follow PEP 8
- Use type hints
- Write docstrings
- Comment complex logic

---

## ğŸ“š Learning Resources

### RAG Concepts
- [LangChain RAG Tutorial](https://python.langchain.com/docs/use_cases/question_answering/)
- [ChromaDB Documentation](https://docs.trychroma.com/)
- [Vector Databases Explained](https://www.pinecone.io/learn/vector-database/)

### LLM Integration
- [OpenAI API Docs](https://platform.openai.com/docs/)
- [Groq API Docs](https://console.groq.com/docs/)
- [Google Gemini Docs](https://ai.google.dev/docs/)

### Advanced Topics
- [Prompt Engineering](https://platform.openai.com/docs/guides/prompt-engineering)
- [Retrieval Strategies](https://arxiv.org/abs/2312.10997)
- [LLM Evaluation](https://github.com/openlifeScienceAI/ragger)

---

## ğŸ“„ License

This project is licensed under the **Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License** (CC BY-NC-SA 4.0) - see [LICENSE](LICENSE) file for details.

**Key Points**:
- âœ… **Attribution**: You must credit the original authors
- âœ… **Share-Alike**: Any modifications must use the same license
- âŒ **Non-Commercial**: Cannot be used for commercial purposes
- âœ… **Modification**: You can modify the code

**What you CAN do**:
- Use for educational purposes
- Use in academic projects
- Use in non-commercial research
- Modify for personal use
- Share modifications (with same license)

**What you CANNOT do**:
- âŒ Use commercially
- âŒ Sell the software
- âŒ Use in commercial products
- âŒ Change the license

For the full license text, see [LICENSE](LICENSE) file.

```
Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License

This work is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.
To view a copy of this license, visit http://creativecommons.org/licenses/by-nc-sa/4.0/
```

---

## ğŸ“ Author

**Your Name** - AAIDC Project Contributor

- ğŸ“§ Email: your.email@example.com
- ğŸ™ GitHub: [@yourusername](https://github.com/yourusername)
- ğŸ’¼ LinkedIn: [Your Profile](https://linkedin.com/in/yourprofile)

---

## ğŸ™ Acknowledgments

- [LangChain](https://langchain.com/) - LLM orchestration framework
- [ChromaDB](https://www.trychroma.com/) - Vector database
- [Groq](https://groq.com/) - Fast LLM inference
- [OpenAI](https://openai.com/) - GPT models
- [Google](https://ai.google.dev/) - Gemini models

---

## ğŸ“ Support

Need help? Here are your options:

1. **Check Documentation**: Read this README and config files
2. **Review Examples**: Check `tests/` for usage examples
3. **Search Issues**: Look for similar issues on GitHub
4. **Create Issue**: If problem persists, create a GitHub issue
5. **Discussions**: Join community discussions on GitHub

---

**Last Updated**: January 2026
**Status**: âœ… Production Ready | 191 Tests Passing | 78% Coverage
